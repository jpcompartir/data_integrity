---
title: "Internal data collection"
format:
  html:
    toc-location: left
    toc-depth: 4
    theme: lux
    highlight-style: espresso
execute:
  echo: false
  message: false
  warning: false
---

```{r}
library(tidyverse)
library(arrow)
library(ParseR)
library(here)
```

# Schema

id | text | label | label_text | source | label_confirmed
 
# Publicly Available

Kaggle dataset takes from [Spam Assassin](https://spamassassin.apache.org/old/publiccorpus/) which was created in the early 2000's, so we could do with something more modern...

The Kaggle Dataset has some pre-processing already baked in e.g. numbers -> NUMBER.
```{r}
kaggle_spam <- read_csv("~/Downloads/spam_or_not_spam.csv") %>%
  filter(!is.na(email))
```

Dataset is imbalanced, 5 -> 1 in terms of not_spam vs spam. 
```{r}
kaggle_spam %>%
  count(label)
```

Re-format and add an id column
```{r}
kaggle_spam <- kaggle_spam %>%
  rename(text = email) %>%
  mutate(source = "kaggle_spam",
         label_text = case_match(label,
                            1 ~ "spam",
                            0 ~ "not_spam"),
         id = paste0("kaggle_", row_number()),
         label_confirmed = "no") %>%
  relocate(id, text, label, label_text, source)

# write_csv(kaggle_spam, here("data/corpus/kaggle_spam.csv"))
```

# Internal
```{r}
msft_dir <- "~/Google Drive/My Drive/data_science_project_work/microsoft/project_work/"
```

Can't just grep inside the data_science_project_work/microsoft/project_work folder because there's too much data in there and we'd have to download it all. So we'll go through selectively and get some data.

```{r, edge_758}
edge_758_spam <- read_parquet("~/Google Drive/My Drive/data_science_project_work/microsoft/project_work/758_microsoft_perceptions_landscape/data/parquet_files/edge_deleted_spam/edge_deleted_spam_2024-06-18 11:11:37.248143.parquet")

edge_758_spam <- edge_758_spam %>%
  mutate(string_length = str_length(message))

edge_758_spam %>%
  summarise(
    mean = mean(string_length, na.rm = TRUE),
    max = max(string_length, na.rm = TRUE),
    min = min(string_length, na.rm = TRUE),
    median = median(string_length, na.rm = TRUE)
  )

edge_758_spam <- edge_758_spam %>% filter(string_length < 5000)

edge_758_spam %>%
  select( text = message) %>%
  mutate(label = "") %>%
  slice_sample(n = 5000) #%>%
  # write_csv("~/Documents/doccano_test.csv")

edge_758_spam %>%
  count(social_network)

edge_758_spam %>%
  filter(social_network != "WEB") %>%
  select(text = message) %>%
  mutate(label = "") %>% 
  filter(str_detect(text, "Improve to Microsoft Edge "))
  sample_n(2000) #%>%
  # write_csv("~/Documents/doccano_edge_spam_not_web.csv")
```

Sample some outputs to see what type of spam we're dealing with.
```{r, edge_trigrams}
edge_758_spam %>%
  select(universal_message_id, created_time, message, permalink) %>%
  # relocate(message) %>%
  sample_n(10) %>%
  # pull(message) %>%
  select(message) %>%
  DT::datatable()
  
edge_758_spam %>%
  count(message, sort = TRUE)

edge_758_trispams <- edge_758_spam %>%
  # sample_n(10) %>%
  tidytext::unnest_ngrams(n = 3L, output = ngrams, input = message) %>%
  count(ngrams, sort = TRUE)

edge_4grams <- edge_758_spam %>%
  # sample_n(10) %>%
  tidytext::unnest_ngrams(n = 4L, output = ngrams, input = message) %>%
  count(ngrams, sort = TRUE)

edge_4grams %>%
  head(1000) %>%
  DT::datatable()

with_edge_4grams <- edge_4grams %>% 
  filter(str_detect(ngrams, "\\bedge"))

with_edge_4grams %>%
  filter(n > 20) %>%
  DT::datatable()

other_browsers <- edge_758_spam %>%
  filter(!str_detect(sender_screen_name, "RSS")) %>%
  filter(str_detect(message, "Edge or another browser"))

other_browsers %>%
  select(message, sender_screen_name) %>%
  DT::datatable()

to_edge_to <- edge_758_spam %>%
  mutate(message = tolower(message)) %>%
  filter(str_detect(message, "to microsoft edge to"))

to_edge_to %>%
  filter(!str_detect(sender_screen_name, "RSS")) %>%
  sample_n(10) %>% 
  select(message, sender_screen_name) %>%
  DT::datatable()
  

to_edge_to %>%
  LimpiaR::limpiar_spam_grams(message, 6, 5)
```
From this batch there are 'exam practice, exam questions, exam dumps' which centre together. 'template site' , 'line casino'.

One issue currently is that it's unclear what parameters are used in practice for the limpiar_spam_grams() functions, and it's unclear to what extent long posts are removed due to the count of n_gram sequences within their own document. This shouldn't be much of an issue if n is 'high enough', but maybe some type of composite measurement of 'total count' and 'in distinct posts'. Or encouraging the chunking of long documents.

If using low values of n e.g. 4 on a Microsoft Edge dataset, the terms Microsoft Edge or MS edge will take up 2, so it's 1 word either side or 2 words before/after.Each n_gram sequence may represent a tiny fraction of the overall dataset in isolation, but, depending on the research question, this may removal all (or nearly all) of the relevant data. 

There are also RSS feeds which are clogging up the datasets it appears.
```{r, edge_rss}
edge_758_trispams %>%
  filter(n > 100) %>%
  DT::datatable()
  
edge_758_spam %>% 
  filter(str_detect(message, "stability updates")) %>%
  select(message, sender_screen_name, created_time) %>%
  count(sender_screen_name, sort = TRUE)

edge_758_spam %>%
  mutate(sender_screen_name = 
           ifelse(
             is.na(sender_screen_name),
                   "NA", 
                   sender_screen_name)) %>%
  filter(!str_detect(sender_screen_name, "RSS feed"))
```

```{r}
edge_758_spam %>%
  filter(str_detect(sender_screen_name, "RSS feed")) %>%
  sample_n(10) %>%
  pull(message)
```


# Synthetic Data

Spam definition prompt - we want an LLM to help us generate a spam on a wide-range of topics and subtopics so that our corpus is linguistically diverse. Using synthetic data generation ought to be significantly faster than manual tagging.

Generations:
Topics -> 10 given by us
Subtopics -> 10 generated by each LLM for each topic

Prompt to write a document for each topic/subtopic/spam vs not spam 

Brands -> 10 most common per subtopic

```{r}
reticulate::use_condaenv("openai")
```


[HF repo with synthetic instruction-dataset generation](https://huggingface.co/datasets/bigcode/self-oss-instruct-sc2-exec-filter-50k?row=0)

[Meta text quality heuristics](https://arxiv.org/pdf/2405.01582) - Heuristics for cleaning massive corpora of internet text data ready for training with LLMs.
[Nemotron Technical Report](https://d1qx31qr3h6wln.cloudfront.net/publications/Nemotron_4_340B_8T_0.pdf) - Large model released and open-sourced by Nvidia with many tricks for synthetic data generation



