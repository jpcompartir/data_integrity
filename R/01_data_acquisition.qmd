---
title: "Internal data collection"
format:
  html:
    toc-location: left
    toc-depth: 4
    theme: lux
    highlight-style: espresso
execute:
  echo: false
  message: false
  warning: false
---

```{r}
library(tidyverse)
```

# Schema

id | text | label | label_text | source | label_confirmed
 
# Publicly Available

Kaggle dataset takes from [Spam Assassin](https://spamassassin.apache.org/old/publiccorpus/) which was created in the early 2000's, so we could do with something more modern...

The Kaggle Dataset has some pre-processing already baked in e.g. numbers -> NUMBER.
```{r}
kaggle_spam <- read_csv("~/Downloads/spam_or_not_spam.csv") %>%
  filter(!is.na(email))
```

Dataset is imbalanced, 5 -> 1 in terms of not_spam vs spam. 
```{r}
kaggle_spam %>%
  count(label)
```

Re-format and add an id column
```{r}
kaggle_spam <- kaggle_spam %>%
  rename(text = email) %>%
  mutate(source = "kaggle_spam",
         label_text = case_match(label,
                            1 ~ "spam",
                            0 ~ "not_spam"),
         id = paste0("kaggle_", row_number()),
         label_confirmed = "no") %>%
  relocate(id, text, label, label_text, source)

# write_csv(kaggle_spam, here("data/corpus/kaggle_spam.csv"))
```

# Internal
Can't just grep inside the data_science_project_work/microsoft/project_work folder because there's too much data in there and we'd have to download it all. So we'll go through selectively and get some data.

```{r}
msft_dir <- "~/Google Drive/My Drive/data_science_project_work/microsoft/project_work/"

library(arrow)
edge_758_spam <- read_parquet("~/Google Drive/My Drive/data_science_project_work/microsoft/project_work/758_microsoft_perceptions_landscape/data/parquet_files/edge_deleted_spam/edge_deleted_spam_2024-06-18 11:11:37.248143.parquet")

edge_758_spam <- edge_758_spam %>%
  mutate(string_length = str_length(message))

edge_758_spam %>%
  summarise(
    mean = mean(string_length, na.rm = TRUE),
    max = max(string_length, na.rm = TRUE),
    min = min(string_length, na.rm = TRUE),
    median = median(string_length, na.rm = TRUE)
  )

edge_758_spam <- edge_758_spam %>% filter(string_length < 5000)
```

Sample some outputs to see what type of spam we're dealing with.
```{r}
edge_758_spam %>%
  select(universal_message_id, created_time, message, permalink) %>%
  # relocate(message) %>%
  sample_n(10) %>%
  # pull(message) %>%
  select(message) %>%
  DT::datatable()
  

```

# Synthetic Data

Spam definition prompt - we want an LLM to help us generate a spam on a wide-range of topics and subtopics so that our corpus is linguistically diverse. Using synthetic data generation ought to be significantly faster than manual tagging.

Generations:
Topics -> 10 given by us
Subtopics -> 10 generated by each LLM for each topic

Prompt to write a document for each topic/subtopic/spam vs not spam 

Brands -> 10 most common per subtopic

